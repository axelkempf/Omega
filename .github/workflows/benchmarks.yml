name: Benchmarks

on:
  push:
    branches: [main]
    paths:
      - 'src/backtest_engine/core/**'
      - 'src/backtest_engine/rating/**'
      - 'tests/benchmarks/**'
  pull_request:
    paths:
      - 'src/backtest_engine/core/**'
      - 'src/backtest_engine/rating/**'
      - 'tests/benchmarks/**'
  workflow_dispatch:
    inputs:
      compare_baseline:
        description: 'Compare against stored baseline'
        required: false
        default: true
        type: boolean
      save_baseline:
        description: 'Save results as new baseline'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  pull-requests: write

env:
  BENCHMARK_OUTPUT_DIR: .benchmark_results

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # FÃ¼r Git History und Baseline-Vergleich

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -e .[dev]
          pip install pytest-benchmark

      - name: Create output directory
        run: mkdir -p ${{ env.BENCHMARK_OUTPUT_DIR }}

      - name: Run benchmark suite
        id: benchmarks
        run: |
          # Run benchmarks with JSON output
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_results.json \
            --benchmark-columns=min,max,mean,stddev,rounds \
            --benchmark-sort=name \
            -v
        continue-on-error: true

      - name: Process benchmark results
        id: process
        run: |
          python << 'EOF'
          import json
          import os
          from pathlib import Path

          results_file = Path("${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_results.json")

          if not results_file.exists():
              print("::warning::No benchmark results found")
              exit(0)

          with open(results_file) as f:
              data = json.load(f)

          benchmarks = data.get("benchmarks", [])
          if not benchmarks:
              print("::warning::No benchmarks in results")
              exit(0)

          # Generate summary
          summary = []
          summary.append("## ðŸ“Š Benchmark Results\n")
          summary.append("| Benchmark | Mean (ms) | StdDev (ms) | Min (ms) | Max (ms) | Rounds |")
          summary.append("|-----------|-----------|-------------|----------|----------|--------|")

          for b in sorted(benchmarks, key=lambda x: x["name"]):
              name = b["name"]
              stats = b["stats"]
              mean_ms = stats["mean"] * 1000
              stddev_ms = stats["stddev"] * 1000
              min_ms = stats["min"] * 1000
              max_ms = stats["max"] * 1000
              rounds = stats["rounds"]
              summary.append(f"| {name} | {mean_ms:.3f} | {stddev_ms:.3f} | {min_ms:.3f} | {max_ms:.3f} | {rounds} |")

          summary_text = "\n".join(summary)

          # Write summary to file
          summary_file = Path("${{ env.BENCHMARK_OUTPUT_DIR }}/summary.md")
          summary_file.write_text(summary_text)

          # Output for GitHub Actions
          print(f"benchmark_count={len(benchmarks)}")

          # Check for regressions (>20% slower than baseline)
          baseline_file = Path("reports/performance_baselines/benchmark_baseline.json")
          regressions = []

          if baseline_file.exists():
              with open(baseline_file) as f:
                  baseline_data = json.load(f)

              baseline_benchmarks = {b["name"]: b for b in baseline_data.get("benchmarks", [])}

              for b in benchmarks:
                  name = b["name"]
                  if name in baseline_benchmarks:
                      baseline_mean = baseline_benchmarks[name]["stats"]["mean"]
                      current_mean = b["stats"]["mean"]
                      change_pct = ((current_mean - baseline_mean) / baseline_mean) * 100

                      if change_pct > 20:
                          regressions.append({
                              "name": name,
                              "baseline_ms": baseline_mean * 1000,
                              "current_ms": current_mean * 1000,
                              "change_pct": change_pct
                          })

          if regressions:
              print("::warning::Performance regressions detected!")
              for r in regressions:
                  print(f"  - {r['name']}: {r['baseline_ms']:.3f}ms -> {r['current_ms']:.3f}ms (+{r['change_pct']:.1f}%)")

          # Write regressions to output
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"regression_count={len(regressions)}\n")
              f.write(f"benchmark_count={len(benchmarks)}\n")

          EOF

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.python-version }}
          path: ${{ env.BENCHMARK_OUTPUT_DIR }}/
          retention-days: 30

      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summaryPath = '${{ env.BENCHMARK_OUTPUT_DIR }}/summary.md';

            if (!fs.existsSync(summaryPath)) {
              console.log('No summary file found');
              return;
            }

            const summary = fs.readFileSync(summaryPath, 'utf8');
            const regressionCount = ${{ steps.process.outputs.regression_count || 0 }};

            let body = summary;

            if (regressionCount > 0) {
              body += '\n\nâš ï¸ **Warning:** ' + regressionCount + ' performance regression(s) detected (>20% slower than baseline)';
            } else {
              body += '\n\nâœ… No significant performance regressions detected';
            }

            body += '\n\n---\n*Generated by benchmark workflow*';

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('Generated by benchmark workflow')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Save baseline (manual trigger only)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.save_baseline == 'true'
        run: |
          mkdir -p reports/performance_baselines
          cp ${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_results.json reports/performance_baselines/benchmark_baseline.json
          echo "Baseline saved at $(date -u +%Y-%m-%dT%H:%M:%SZ)" > reports/performance_baselines/baseline_info.txt

      - name: Check for regressions
        if: steps.process.outputs.regression_count > 0
        run: |
          echo "::warning::${{ steps.process.outputs.regression_count }} performance regression(s) detected"
          # Don't fail the build, just warn
          exit 0


  property-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -e .[dev]
          pip install hypothesis

      - name: Run property-based tests
        run: |
          pytest tests/property/ \
            -v \
            --hypothesis-profile=ci \
            --tb=short
        env:
          HYPOTHESIS_PROFILE: ci


  golden-file-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -e .[dev]

      - name: Run golden-file tests
        run: |
          pytest tests/golden/ \
            -v \
            --tb=short
