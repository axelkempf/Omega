name: Benchmarks

on:
  push:
    branches: [main]
    paths:
      - 'src/backtest_engine/core/**'
      - 'src/backtest_engine/rating/**'
      - 'tests/benchmarks/**'
  pull_request:
    paths:
      - 'src/backtest_engine/core/**'
      - 'src/backtest_engine/rating/**'
      - 'tests/benchmarks/**'
  workflow_dispatch:
    inputs:
      compare_baseline:
        description: 'Compare against stored baseline'
        required: false
        default: true
        type: boolean
      save_baseline:
        description: 'Save current results as new baseline'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  actions: read
  pull-requests: write

env:
  BENCHMARK_OUTPUT_DIR: .benchmark_results
  # Baseline-Vergleich ist per default aktiv. Workflow-Dispatch kann ihn explizit deaktivieren.
  # - pull_request: immer "true" (harte Gate-Policy)
  # - push (main): "false" (Bootstrap/Artefakt-Erzeugung ohne Catch-22)
  # - workflow_dispatch: input (default: true)
  COMPARE_BASELINE: ${{ github.event_name == 'pull_request' && 'true' || github.event_name == 'push' && 'false' || (github.event.inputs.compare_baseline && 'true' || 'false') }}
  # Optional: repo-gepinnte Baseline (falls ihr sie versionieren wollt)
  REPO_BASELINE_FILE: reports/performance_baselines/benchmark_baseline.json
  # Default-Baseline aus dem letzten erfolgreichen main-Run (Artifact)
  MAIN_BASELINE_FILE: .benchmark_results/main_baseline_benchmark_results.json

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # FÃ¼r Git History und Baseline-Vergleich

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -e .[dev]
          pip install pytest-benchmark

      - name: Create output directory
        run: mkdir -p ${{ env.BENCHMARK_OUTPUT_DIR }}

      - name: Download baseline from latest successful main run (artifact)
        if: env.COMPARE_BASELINE == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          PYTHON_VERSION: ${{ matrix.python-version }}
        run: |
          python << 'EOF'
          from __future__ import annotations

          import io
          import json
          import os
          import sys
          import zipfile
          from pathlib import Path
          from urllib.request import Request, urlopen

          compare = os.environ.get("COMPARE_BASELINE") == "true"
          if not compare:
            print("Baseline compare disabled")
            sys.exit(0)

          token = os.environ.get("GH_TOKEN")
          if not token:
            print("::error::Missing token for GitHub API (GITHUB_TOKEN)")
            sys.exit(1)

          repo = os.environ.get("GITHUB_REPOSITORY")
          if not repo:
            print("::error::Missing GITHUB_REPOSITORY")
            sys.exit(1)

          python_version = os.environ.get("PYTHON_VERSION", "3.12")
          artifact_name = f"benchmark-results-{python_version}"
          baseline_target = Path(os.environ.get("MAIN_BASELINE_FILE", ".benchmark_results/main_baseline_benchmark_results.json"))
          baseline_target.parent.mkdir(parents=True, exist_ok=True)

          def api_get_json(url: str) -> dict:
            req = Request(url)
            req.add_header("Authorization", f"Bearer {token}")
            req.add_header("Accept", "application/vnd.github+json")
            with urlopen(req) as resp:  # noqa: S310
              return json.loads(resp.read().decode("utf-8"))

          def api_get_bytes(url: str) -> bytes:
            req = Request(url)
            req.add_header("Authorization", f"Bearer {token}")
            req.add_header("Accept", "application/vnd.github+json")
            with urlopen(req) as resp:  # noqa: S310
              return resp.read()

          # Latest successful run of this workflow on main
          runs_url = f"https://api.github.com/repos/{repo}/actions/workflows/benchmarks.yml/runs?branch=main&status=success&per_page=5"
          runs = api_get_json(runs_url).get("workflow_runs", [])
          if not runs:
            print(
              "::warning::No successful baseline run found on main for benchmarks.yml. "
              "Skipping baseline compare for this run (bootstrap mode)."
            )
            sys.exit(0)

          run_id = runs[0]["id"]
          artifacts_url = f"https://api.github.com/repos/{repo}/actions/runs/{run_id}/artifacts?per_page=100"
          artifacts = api_get_json(artifacts_url).get("artifacts", [])

          artifact = next((a for a in artifacts if a.get("name") == artifact_name), None)
          if not artifact:
            names = ", ".join(sorted(a.get("name", "") for a in artifacts))
            print(
              f"::warning::Baseline artifact '{artifact_name}' not found on latest successful main run. "
              f"Available artifacts: {names}. Skipping baseline compare for this run."
            )
            sys.exit(0)

          artifact_id = artifact["id"]
          zip_url = f"https://api.github.com/repos/{repo}/actions/artifacts/{artifact_id}/zip"
          zip_bytes = api_get_bytes(zip_url)

          with zipfile.ZipFile(io.BytesIO(zip_bytes)) as zf:
            # The uploaded artifact contains the contents of BENCHMARK_OUTPUT_DIR.
            candidates = [n for n in zf.namelist() if n.endswith("benchmark_results.json")]
            if not candidates:
              print("::warning::Baseline artifact does not contain benchmark_results.json. Skipping baseline compare for this run.")
              sys.exit(0)
            # Prefer the shortest path (usually root-level)
            candidates.sort(key=len)
            with zf.open(candidates[0]) as f:
              baseline_target.write_bytes(f.read())

          print(f"âœ“ Baseline downloaded from main: run_id={run_id}, artifact={artifact_name} -> {baseline_target}")
          EOF

      - name: Run benchmark suite
        id: benchmarks
        run: |
          # Run benchmarks with JSON output
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_results.json \
            --benchmark-columns=min,max,mean,stddev,rounds \
            --benchmark-sort=name \
            -v

      - name: Process benchmark results
        id: process
        run: |
          python << 'EOF'
          import json
          import os
          from pathlib import Path

          results_file = Path("${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_results.json")

          if not results_file.exists():
              print("::error::No benchmark results found (expected JSON output from pytest-benchmark)")
              exit(1)

          with open(results_file) as f:
              data = json.load(f)

          benchmarks = data.get("benchmarks", [])
          if not benchmarks:
              print("::error::No benchmarks in results (no evidence produced)")
              exit(1)

          # Generate summary
          summary = []
          summary.append("## ðŸ“Š Benchmark Results\n")
          summary.append("| Benchmark | Mean (ms) | StdDev (ms) | Min (ms) | Max (ms) | Rounds |")
          summary.append("|-----------|-----------|-------------|----------|----------|--------|")

          for b in sorted(benchmarks, key=lambda x: x["name"]):
              name = b["name"]
              stats = b["stats"]
              mean_ms = stats["mean"] * 1000
              stddev_ms = stats["stddev"] * 1000
              min_ms = stats["min"] * 1000
              max_ms = stats["max"] * 1000
              rounds = stats["rounds"]
              summary.append(f"| {name} | {mean_ms:.3f} | {stddev_ms:.3f} | {min_ms:.3f} | {max_ms:.3f} | {rounds} |")

          summary_text = "\n".join(summary)

          # Write summary to file
          summary_file = Path("${{ env.BENCHMARK_OUTPUT_DIR }}/summary.md")
          summary_file.write_text(summary_text)

          # Output for GitHub Actions
          print(f"benchmark_count={len(benchmarks)}")

          # Check for regressions (>20% slower than baseline)
          compare_baseline = os.environ.get("COMPARE_BASELINE") == "true"

          repo_baseline_file = Path(
              os.environ.get(
                  "REPO_BASELINE_FILE", "reports/performance_baselines/benchmark_baseline.json"
              )
          )
          main_baseline_file = Path(
              os.environ.get(
                  "MAIN_BASELINE_FILE", ".benchmark_results/main_baseline_benchmark_results.json"
              )
          )

          baseline_file = repo_baseline_file if repo_baseline_file.exists() else main_baseline_file
          regressions: list[dict] = []

          baseline_source = "none"

          if compare_baseline and not baseline_file.exists():
              print(
                "::warning::Baseline compare enabled, but no baseline available.\n"
                f"Checked: {repo_baseline_file} (repo) and {main_baseline_file} (downloaded from main).\n"
                "Skipping regression checks for this run (bootstrap mode)."
              )
              compare_baseline = False

          if baseline_file.exists():
              baseline_source = (
                  "repo"
                  if baseline_file.resolve() == repo_baseline_file.resolve()
                  else "main-artifact"
              )
              with open(baseline_file) as f:
                  baseline_data = json.load(f)

              baseline_benchmarks = {b["name"]: b for b in baseline_data.get("benchmarks", [])}

              for b in benchmarks:
                  name = b["name"]
                  if name in baseline_benchmarks:
                      baseline_mean = baseline_benchmarks[name]["stats"]["mean"]
                      current_mean = b["stats"]["mean"]
                      change_pct = ((current_mean - baseline_mean) / baseline_mean) * 100

                      if change_pct > 20:
                          regressions.append(
                              {
                                  "name": name,
                                  "baseline_ms": baseline_mean * 1000,
                                  "current_ms": current_mean * 1000,
                                  "change_pct": change_pct,
                              }
                          )

          if regressions:
              print("::warning::Performance regressions detected!")
              for r in regressions:
                  print(f"  - {r['name']}: {r['baseline_ms']:.3f}ms -> {r['current_ms']:.3f}ms (+{r['change_pct']:.1f}%)")

              regressions_txt = Path("${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_regressions.txt")
              lines = [
                f"{r['name']}: {r['baseline_ms']:.3f}ms -> {r['current_ms']:.3f}ms (+{r['change_pct']:.1f}%)"
                for r in regressions
              ]
              regressions_txt.write_text("\n".join(lines) + "\n")

          # Write regressions to output
          with open(os.environ["GITHUB_OUTPUT"], "a") as f:
              f.write(f"regression_count={len(regressions)}\n")
              f.write(f"benchmark_count={len(benchmarks)}\n")
              f.write(f"baseline_source={baseline_source}\n")
              f.write(f"compare_baseline={'true' if compare_baseline else 'false'}\n")

          EOF

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.python-version }}
          path: ${{ env.BENCHMARK_OUTPUT_DIR }}/
          retention-days: 30

      - name: Comment on PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summaryPath = '${{ env.BENCHMARK_OUTPUT_DIR }}/summary.md';

            if (!fs.existsSync(summaryPath)) {
              console.log('No summary file found');
              return;
            }

            const summary = fs.readFileSync(summaryPath, 'utf8');
            const regressionCount = parseInt('${{ steps.process.outputs.regression_count }}' || '0', 10);
            const compareBaseline = ('${{ steps.process.outputs.compare_baseline }}' || 'false') === 'true';
            const baselineSource = '${{ steps.process.outputs.baseline_source }}' || 'none';

            let body = summary;

            if (!compareBaseline) {
              body += '\n\nâ„¹ï¸ Baseline comparison disabled for this run.';
            } else if (baselineSource === 'none') {
              body += '\n\nâŒ Baseline comparison was enabled but no baseline was available (job should fail).';
            } else if (regressionCount > 0) {
              body += '\n\nâš ï¸ **Warning:** ' + regressionCount + ' performance regression(s) detected (>20% slower than baseline)';
            } else {
              body += '\n\nâœ… No significant performance regressions detected';
            }

            if (compareBaseline && baselineSource !== 'none') {
              body += `\n\nBaseline source: **${baselineSource}**`;
            }

            body += '\n\n---\n*Generated by benchmark workflow*';

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('Generated by benchmark workflow')
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Save baseline (manual trigger only)
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.save_baseline == 'true'
        run: |
          mkdir -p reports/performance_baselines
          cp ${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_results.json reports/performance_baselines/benchmark_baseline.json
          echo "Baseline saved at $(date -u +%Y-%m-%dT%H:%M:%SZ)" > reports/performance_baselines/baseline_info.txt

      - name: Check for regressions
        if: steps.process.outputs.regression_count != '0'
        run: |
          echo "::error::${{ steps.process.outputs.regression_count }} performance regression(s) detected (>20% slower than baseline)"
          if [ -f "${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_regressions.txt" ]; then
            cat "${{ env.BENCHMARK_OUTPUT_DIR }}/benchmark_regressions.txt"
          fi
          exit 1


  property-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -e .[dev]
          pip install hypothesis

      - name: Run property-based tests
        run: |
          pytest tests/property/ \
            -v \
            --hypothesis-profile=ci \
            --tb=short
        env:
          HYPOTHESIS_PROFILE: ci


  golden-file-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -e .[dev]

      - name: Run golden-file tests
        run: |
          pytest tests/golden/ \
            -v \
            --tb=short
